# llm_engine.py

from typing import List, Optional, Dict, Any
import os
import json
import re
import unicodedata
import logging
from difflib import get_close_matches

from dotenv import load_dotenv
from langchain.chains import LLMChain
from langchain.memory import ConversationBufferMemory, ConversationEntityMemory, CombinedMemory
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.prompts import PromptTemplate

from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.chat_models import ChatOpenAI
from langchain.memory import ConversationBufferMemory, ConversationEntityMemory
from openai import OpenAI
from app.schemas import UserProfile, SessionState
# Chargement des variables d‚Äôenvironnement (cl√© API OpenAI, etc.)

load_dotenv(dotenv_path="app/.env")
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Initialisation du client OpenAI pour les appels directs (classification d'intention, d√©tection de titre)

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

class LLMEngine:
    def __init__(self, df_formations, session: Optional[SessionState] = None):
        """
        Initialise le moteur LLM avec embeddings, mod√®le LLM et base vectorielle.
        Charge les donn√©es de formations et pr√©pare la cha√Æne RAG.
        """
        # Initialisation des embeddings OpenAI pour la vectorisation du texte
        self.embeddings = OpenAIEmbeddings()
        # Mod√®le ChatOpenAI pour la g√©n√©ration de r√©ponses (ton l√©g√®rement al√©atoire pour style commercial)
        self.llm = ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0.7)
        # Stockage vectoriel (sera initialis√© plus loin)
        self.vector_store = None
        # Cha√Æne QA conversationnelle (non utilis√©e directement dans la nouvelle version, remplac√©e par logique manuelle)
        self.qa_chain = None
        self.session = session or SessionState(user_id="default")
        # Donn√©es et m√©moire interne
        self.formations_json = {}       # Dictionnaire des formations (titre -> donn√©es JSON)
        self.all_documents = []         # Documents LangChain pour chaque formation
        self.titles_list = []           # Liste des titres de formation disponibles (en minuscules)
        self.titles_joined = ""         # Titres joints par sauts de ligne (pour prompt d√©tection)
        #self.current_title = None       # Titre de formation courant (en minuscules) suivi durant la conversation

        # Initialisation de la base RAG (documents + vecteurs)
        self.initialize_rag(df_formations)

    def _df_to_documents(self, df) -> List[Document]:
        """
        Convertit le DataFrame des formations en une liste de Documents LangChain.
        Chaque document contient le contenu texte d'une formation (titre, objectifs, etc.),
        et les m√©tadonn√©es correspondantes pour le filtrage.
        """
        docs = []
        for _, row in df.iterrows():
            # Construction du contenu textuel combinant les principales rubriques de la formation
            content = (
                f"Formation: {row['titre']}\n"
                f"Objectifs: {', '.join(row['objectifs'])}\n"
                f"Pr√©requis: {', '.join(row['prerequis'])}\n"
                f"Programme: {', '.join(row['programme'])}\n"
                f"Public: {', '.join(row['public'])}\n"
                f"Lien: {row['lien']}\n"
                f"Dur√©e: {row.get('dur√©e', '')}\n"
                f"Tarif: {row.get('tarif', '')}\n"
                f"Modalit√©: {row.get('modalit√©', '')}\n"
            )
            # Cr√©ation du Document avec contenu et m√©tadonn√©es
            docs.append(Document(
                page_content=content,
                metadata={
                    "titre": row["titre"],        # Titre exact de la formation
                    "type": "formation",
                    "niveau": row.get("niveau", ""),
                    "modalite": row.get("modalit√©", ""),
                    "duree": row.get("dur√©e", ""),
                    "tarif": row.get("tarif", ""),
                    "objectifs": ", ".join(row.get("objectifs", [])),
                    "prerequis": ", ".join(row.get("prerequis", [])),
                    "programme": ", ".join(row.get("programme", [])),
                    "public": ", ".join(row.get("public", []))
                }
            ))
        return docs

    def initialize_rag(self, df_formations):
        """
        Initialise le syst√®me RAG:
        - Charge les documents de formation.
        - Construit la base vectorielle Chroma des documents.
        - Charge les donn√©es JSON compl√®tes des formations pour acc√®s direct.
        """
        # Conversion du DataFrame en Documents et stockage
        documents = self._df_to_documents(df_formations)
        self.all_documents = documents

        # Chargement des donn√©es JSON compl√®tes de chaque formation (pour r√©ponses directes)
        for file in os.listdir("./app/content"):
            if file.endswith(".json"):
                with open(os.path.join("./app/content", file), "r", encoding="utf-8") as f:
                    data = json.load(f)
                    titre = data.get("titre", "")
                    if titre:
                        # On stocke la formation avec un indice lowercase pour correspondance simplifi√©e
                        self.formations_json[titre.lower()] = data

        # Pr√©paration des titres de formations disponibles (pour d√©tection via LLM)
        self.titles_list = list(self.formations_json.keys())  # en minuscules
        self.titles_joined = "\n".join(self.titles_list)

        # D√©coupage des documents en chunks pour une meilleure vectorisation (limite 1000 caract√®res, chevauchement 200)
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(documents)

        # Initialisation de la base vectorielle Chroma avec persistance locale
        self.vector_store = Chroma.from_documents(
            documents=splits,
            embedding=self.embeddings,
            persist_directory="./chroma_db"
        )

        # (Optionnel) Initialisation d'une cha√Æne de QA conversationnelle standard 
        # Note: Dans cette refonte, on utilisera une approche manuelle pour plus de contr√¥le
        self.qa_chain = None  # ConversationalRetrievalChain.from_llm(self.llm, self.vector_store.as_retriever(search_kwargs={"k": 15}), return_source_documents=True)

        print("Base de connaissances vectorielle initialis√©e (%d documents).", len(splits))

    def _normalize_text(self, text: str) -> str:
        """
        Normalise une cha√Æne de caract√®res: suppression des accents, conversion en minuscules, et trim des espaces.
        Utile pour la correspondance de textes de fa√ßon robuste.
        """
        return re.sub(r'\s+', ' ', unicodedata.normalize('NFD', text.lower()).encode('ascii', 'ignore').decode("utf-8")).strip()

    def detect_intent(self, question: str) -> str:
        prompt = f"""
            Tu es un classificateur d'intentions. Ton r√¥le est d'identifier l'intention de l'utilisateur √† partir de sa question.

            Les intentions possibles sont :

            - liste_formations
            - recommandation
            - info_objectifs
            - info_prerequis
            - info_programme
            - info_public
            - info_tarif
            - info_duree
            - info_modalite
            - info_certification
            - info_lieu
            - info_prochaine_session
            - none

            Ne r√©ponds qu'avec une seule de ces intentions, sans justification.

            Question : {question}
            """

        response = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content.strip()

    def detect_formation_title(self, question: str) -> str:
        """
        Identifie le titre exact d‚Äôune formation mentionn√©e ou sous-entendue dans la question.
        Utilise un appel LLM en fournissant la liste des titres disponibles.
        Retourne le titre en minuscules si trouv√©, ou 'aucun' si rien de correspondant.
        """
        detect_title_prompt = (
            "Tu es un assistant intelligent.\n\n"
            "Ta t√¢che est de d√©tecter, parmi la liste de titres ci-dessous, le **titre exact** de formation auquel fait r√©f√©rence la question de l'utilisateur.\n"
            "Voici la liste des titres disponibles :\n"
            f"{self.titles_joined}\n\n"
            "R√©ponds uniquement par l'un de ces titres *exactement* comme il appara√Æt dans la liste (pas de phrase compl√®te, pas de guillemets). "
            "Si aucun titre ne correspond, r√©ponds simplement : aucun.\n\n"
            f"Question : {question}\n"
        )
        try:
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": detect_title_prompt}],
                temperature=0
            )
            detected = response.choices[0].message.content.strip()
            detected = detected.lower()
            print("Formation d√©tect√©e par le LLM pour ", detected)
            return detected
        except Exception as e:
            print("Erreur lors de la d√©tection du titre de formation:", e)
            return "aucun"

    def _resolve_pronouns(self, question: str) -> str:
        """
        R√©sout certains pronoms flous dans la question en utilisant le contexte courant (formation mentionn√©e pr√©c√©demment).
        Par exemple, remplace "cette formation", "celle-ci", "celui-ci", "l'autre" par le nom de la formation correspondante si connu.
        """
        if not self.session.current_title:
            # Pas de formation courante connue pour donner du contexte
            return question

        resolved_question = question
        titre_display = self.formations_json.get(self.session.current_title, {}).get("titre", self.session.current_title.title())

        # Pronoms √† remplacer s'ils sont pr√©sents dans la question
        pronoms_flous = ["cette formation", "cette formation-ci", "cette formation l√†", "celle-ci", "celui-ci", "celle la", "celle-l√†", "l'autre formation", "l'autre"]
        for pronom in pronoms_flous:
            if pronom.lower() in resolved_question.lower():
                # Remplacement par "la formation XYZ" pour plus de clart√©
                resolved_question = re.sub(pronom, f"la formation {titre_display}", resolved_question, flags=re.IGNORECASE)
        if resolved_question != question:
            print("Question r√©solue (pronoms remplac√©s) :", resolved_question)
        return resolved_question

    def _is_vague_question(self, question: str) -> bool:
        """
        D√©termine si la question utilisateur est vague/ambig√ºe.
        Une question est consid√©r√©e comme vague si elle est tr√®s courte ou ne contient pas assez de contexte (ex: "et ensuite ?", "comment ?", etc.).
        """
        q = question.strip().lower()
        # Crit√®res simples : longueur tr√®s courte OU phrases communes tr√®s g√©n√©rales
        if len(q) < 5:
            return True
        vague_phrases = ["et ensuite", "ensuite ?", "et apr√®s", "et apres", "comment ?", "pourquoi ?", "et puis"]
        for phrase in vague_phrases:
            if q == phrase or q.endswith(phrase):
                return True
        # Si la question demande une info (pr√©requis, dur√©e, etc.) sans mentionner la formation et qu'on n'a pas de contexte courant
        if self.session.current_title is None:
            mots_clefs = ["objectif", "objectifs", "pr√©requis", "prerequis", "programme", "public", "dur√©e", "duree", "tarif", "modalit√©", "modalite", "certification", "certifiante", "sessions"]
            for mot in mots_clefs:
                if mot in q:
                    # Mot cl√© pr√©sent, pas de formation sp√©cifi√©e => potentiellement vague ("pr√©requis" de quoi ?)
                    return True
        return False

    def generate_response(self, question: str, chat_history: List[tuple], session: SessionState, profile: Optional[Any] = None) -> Dict[str, Any]:
        """
        G√©n√®re la r√©ponse du chatbot pour une question donn√©e, en utilisant l'historique de conversation et le profil utilisateur.
        Ce moteur suit les √©tapes RAG : analyse d'intention, identification de la formation, recherche de contexte, g√©n√©ration de la r√©ponse.
        Il g√®re aussi les questions vagues en fournissant une clarification ou une reformulation si n√©cessaire.
        """
        print(f"üéØ User session ID : {session.user_id}")
        print(f"üéØ Current title (avant traitement) : {self.session.current_title}")

        print(f"Question re√ßue : {question}")
        print(f"Profil utilisateur (recommended_course) : {profile.recommended_course if profile else 'Aucun'}")


        # Pr√©traitement de la question utilisateur
        question = question.strip()
        # On commence par r√©soudre les r√©f√©rences floues (pronoms) en utilisant la formation courante connue
        question = self._resolve_pronouns(question)
        print(f"Question apr√®s r√©solution des pronoms : {question}")

        # D√©tection si la question est trop vague
        if self._is_vague_question(question):
            # Cas d'une question vague/ambigu√´
            if ("ensuite" in question.lower() or "et apr√®s" in question.lower()) and self.session.current_title:
                # Si la question est du type "et ensuite ?" et qu'on a une formation contexte, on reformule automatiquement en question claire
                titre_display = self.formations_json.get(self.session.current_title, {}).get("titre", self.session.current_title.title())
                question = f"Que se passe-t-il apr√®s la formation {titre_display} ?"  # on reformule la question
                print("Reformulation automatique de la question vague en : ", question)
                # On continue le processus avec cette question reformul√©e
            else:
                # Sinon, on g√©n√®re une demande de clarification sans aller plus loin
                if self.session.current_title:
                    # Si on conna√Æt le contexte d'une formation, on demande une pr√©cision sur cette formation
                    titre_display = self.formations_json.get(self.session.current_title, {}).get("titre", self.session.current_title.title())
                    clarification = f"Pouvez-vous pr√©ciser ce que vous souhaitez savoir sur la formation {titre_display} ?"
                else:
                    # Pas de contexte, question trop vague de mani√®re g√©n√©rale
                    clarification = "Pouvez-vous pr√©ciser ce que vous voulez savoir, s'il vous pla√Æt ?"
                return {"answer": clarification}

        # √âtape 1 : D√©tection de l'intention de l'utilisateur
        intent = self.detect_intent(question)
        print(f"Intention d√©tect√©e : {intent}")
        known_intents = {
            "liste_formations", "recommandation",
            "info_objectifs", "info_prerequis", "info_programme",
            "info_public", "info_tarif", "info_duree",
            "info_modalite", "info_certification", "info_lieu", "info_prochaine_session"
        }

        # Intents pouvant n√©cessiter une d√©tection de titre implicite m√™me si ce n'est pas 'recommandation'
       # intents_requ√©rant_titre = {"info_tarif", "info_duree", "info_lieu", "info_certification", "info_modalite", "info_public"}

        if intent in known_intents:
            detected_title = self.detect_formation_title(question)
            print(f"Titre d√©tect√© dans une question '{intent}' : {detected_title}")
            if detected_title and detected_title != "aucun":
                if detected_title in self.formations_json:
                    self.session.current_title = detected_title
                else:
                    close = get_close_matches(detected_title, self.titles_list, n=1, cutoff=0.7)
                    if close:
                        self.session.current_title = close[0]
                print("Titre mis √† jour pour l‚Äôintention info_* :", self.session.current_title)

        
        formation_context = self.formations_json.get(self.session.current_title, {}) if self.session.current_title else {}
        titre_affiche = formation_context.get("titre", "(inconnu)")

        # if not self.session.current_title:
        #     print("‚ùå Aucun titre valide identifi√©, impossible de r√©pondre pr√©cis√©ment.")
        #     return {"answer": "Je n‚Äôai pas identifi√© de formation pr√©cise. Pouvez-vous reformuler ou pr√©ciser le nom de la formation ?" }


        if intent not in known_intents:
            intent = "none"

        # √âtape 2 : Identification du titre de formation mentionn√© ou implicite
        # Par d√©faut, on conserve la formation actuelle si aucune nouvelle formation n'est d√©tect√©e
        if not self.session.current_title:
            matched_title = profile.recommended_course.lower() if profile and profile.recommended_course else None
            if matched_title:
                self.session.current_title = matched_title
                print("Initialisation de current_title depuis le profil recommand√© :", self.session.current_title)
            else:
                print("Aucune formation recommand√©e dans le profil.")
        else:
            print("current_title d√©j√† d√©fini :", self.session.current_title)


        # detected_title = self.detect_formation_title(question)
        # print(f"####\\nFormation d√©tect√©e par LLM : {detected_title}\\n####")

        formation_context = self.formations_json.get(self.session.current_title, {}) if self.session.current_title else {}
        titre_affiche = formation_context.get("titre", "(inconnu)")

        # √âtape 3 : Gestion des intentions particuli√®res avec r√©ponse directe ou traitement sp√©cialis√©

        # 3.a. Intentions d'information directe sur une formation (objectifs, pr√©requis, etc.)
        print(f"Formation courante active : {self.session.current_title}")
        rubriques_info = {
            "info_objectifs": {
                "key": "objectifs",
                "format": lambda val, titre: "Les objectifs de la formation " + titre + " sont :\n- " + "\n- ".join(val) if isinstance(val, list) else f"Objectifs : {val}"
            },
            "info_prerequis": {
                "key": "prerequis",
                "format": lambda val, titre: "Les pr√©requis pour la formation " + titre + " sont :\n- " + "\n- ".join(val) if isinstance(val, list) else f"Pr√©requis : {val}"
            },
            "info_programme": {
                "key": "programme",
                "format": lambda val, titre: "Le programme de la formation " + titre + " contient :\n- " + "\n- ".join(val) if isinstance(val, list) else f"Programme : {val}"
            },
            "info_public": {
                "key": "public",
                "format": lambda val, titre: "La formation " + titre + " s‚Äôadresse √† :\n- " + "\n- ".join(val) if isinstance(val, list) else f"Public vis√© : {val}"
            },
            "info_tarif": {
                "key": "tarif",
                "format": lambda val, titre: f"Le tarif de la formation {titre} est de {val}."
            },
            "info_duree": {
                "key": "dur√©e",
                "format": lambda val, titre: f"La dur√©e de la formation {titre} est de {val}."
            },
            "info_modalite": {
                "key": "modalit√©",
                "format": lambda val, titre: f"La formation {titre} est propos√©e en modalit√© : {val}."
            },
            "info_certification": {
                "key": "certifiant",
                "format": lambda val, titre: f"La formation {titre} est " + ("certifiante." if val=="Oui" else "non certifiante.")
            },
            "info_lieu": {
                "key": "lieu",
                "format": lambda val, titre: f"Le lieu de la formation {titre} est {val}."
            },
            "info_prochaine_session": {
                "key": "prochaines_sessions",
                "format": lambda val, titre: "Les prochaines sessions pr√©vues sont :\n- " + "\n- ".join(val) if isinstance(val, list) else f"Prochaines sessions : {val}"
            }
        }

        
        # On s'assure que intent soit dans les valeurs attendues (sinon on le traitera comme 'none')
        if intent == "info_tarif" and self.session.current_title:
            rubrique = "tarif"
            valeur = formation_context.get(rubrique, "(non renseign√©)")
            if any(mot in question.lower() for mot in ["cher", "co√ªteux", "prix √©lev√©"]):
                justification = formation_context.get("objectifs", "")
                return {
                    "answer": f"Le tarif de la formation {titre_affiche} est de {valeur}. "
                            f"Ce tarif refl√®te les comp√©tences acquises, notamment : "
                            f"{', '.join(justification) if isinstance(justification, list) else justification}"
                }
            else:
                return {"answer": f"Le tarif de la formation {titre_affiche} est de {valeur}."}

        if intent in rubriques_info and self.session.current_title:
            # Si l'utilisateur demande une information sp√©cifique sur la formation courante
            rubrique = rubriques_info[intent]["key"]
            valeur = formation_context.get(rubrique, "(non renseign√©)")
            reponse_directe = rubriques_info[intent]["format"](valeur, titre_affiche)
            return {"answer": reponse_directe}

        # 3.b. Intention de lister toutes les formations
        if intent == "liste_formations":
            titres = [data["titre"] for _, data in self.formations_json.items()]
            liste = "- " + "\n- ".join(titres) if titres else "(Aucune formation disponible)"
            return {"answer": "Voici la liste compl√®te des formations disponibles :\n\n" + liste}

        # 3.c. Intention de recommandation (expliquer ou d√©tailler la formation recommand√©e)
        if intent == "recommandation" and self.session.current_title:
            # Contexte bas√© sur la formation actuelle et le profil pour motiver la recommandation
            objectifs = formation_context.get("objectifs", [])
            public = formation_context.get("public", [])
            prerequis = formation_context.get("prerequis", [])
            objectifs_text = "- " + "\n- ".join(objectifs) if isinstance(objectifs, list) else str(objectifs)
            public_text = "- " + "\n- ".join(public) if isinstance(public, list) else str(public)
            prerequis_text = "- " + "\n- ".join(prerequis) if isinstance(prerequis, list) else str(prerequis)
            context_parts = [
                f"voici les informations g√©n√©rales de la formation **{titre_affiche}**:\n\n",
                f"- Objectifs principaux :\n{objectifs_text}\n",
                f"- Public vis√© :\n{public_text}\n",
                f"- Pr√©requis recommand√©s :\n{prerequis_text}"
            ]
            # On formule la question de l'utilisateur dans ce contexte (s'il y a une question pr√©cise, sinon question g√©n√©rale)
            #user_question = question if intent == "recommandation" and question.lower().startswith("pourquoi") else "Pourquoi cette formation est-elle recommand√©e ?"
            final_prompt = (
                f"Tu es un assistant virtuel sp√©cialis√© dans les formations professionnelles. "
                f"recommande cette formation **{titre_affiche}** et commence par le nom de la formation.\n\n"
                f"Contexte :\n"
                + "\n\n".join(context_parts)
                + f"\n\nQuestion de l'utilisateur : {question}"
            )

            # Appel du LLM sur ce prompt pour g√©n√©rer une r√©ponse justifiant la recommandation
            try:
                result = self.llm.predict(final_prompt)
            except Exception as e:
                print("Erreur lors de la g√©n√©ration de r√©ponse recommandation: ", e)
                return {"answer": "D√©sol√©, je ne parviens pas √† expliquer cette recommandation pour le moment."}
            return {"answer": result}


        if not self.session.current_title:
            print("‚ùå Aucun titre actif d√©tect√©, retour d'une r√©ponse g√©n√©rique.")
            return {"answer": "Je n‚Äôai pas identifi√© de formation pr√©cise. Pouvez-vous pr√©ciser le nom de la formation qui vous int√©resse ?" }

        # √âtape 4 : Cas g√©n√©ral (intent 'none' ou question n√©cessitant recherche dans la base de connaissances)
        # On va utiliser le mod√®le RAG pour formuler une r√©ponse en s'appuyant sur les documents.

        # a. Pr√©paration de la m√©moire conversationnelle (historique + entit√©s)
        # On utilise ConversationBufferMemory et ConversationEntityMemory de LangChain pour reconstituer l'historique en texte et extraire les entit√©s mentionn√©es.
        buffer_memory = ConversationBufferMemory(
            memory_key="history",
            human_prefix="Utilisateur",
            ai_prefix="Assistant",
            return_messages=False
        )
        entity_memory = ConversationEntityMemory(
            llm=ChatOpenAI(model_name="gpt-3.5-turbo", temperature=0),
            memory_key="entities",
            input_key="question"
        )
        # On alimente la m√©moire avec l'historique de la conversation pass√©e (paire utilisateur/assistant)
        for user_msg, assistant_msg in chat_history:
            buffer_memory.chat_memory.add_user_message(user_msg)
            buffer_memory.chat_memory.add_ai_message(assistant_msg)
            try:
                # Mise √† jour de la m√©moire d'entit√©s √† chaque √©change
                entity_memory.save_context({"question": user_msg}, {"output": assistant_msg})
                history_text = buffer_memory.load_memory_variables({}).get("history", "")
                entities_text = entity_memory.load_memory_variables({"question": question}).get("entities", "")


            except Exception as e:
                print("Extraction entit√© √©chou√©e sur ", user_msg, e)
        # R√©cup√©ration des contenus format√©s de l'historique et des entit√©s
        history_text = buffer_memory.load_memory_variables({}).get("history", "")
        entities_text = entity_memory.load_memory_variables({"question": question}).get("entities", "")
        # Si aucune entit√© identifi√©e, on met une valeur par d√©faut pour le prompt
        if not entities_text or entities_text.strip().lower() == "none":
            entities_text = "Aucune"

        # b. Recherche des documents pertinents via Chroma
        # On filtre par formation courante si connue, sinon on cherche globalement
        filter_criteria = {"titre": titre_affiche} if self.session.current_title and titre_affiche else None
        print(f"Filtre de recherche : {filter_criteria}")
        

        try:
            # Requ√™te de recherche vectorielle (on utilise la question non modifi√©e pour la similarit√©)
            docs = self.vector_store.similarity_search(question, k=6, filter=filter_criteria)
            # Supprimer les doublons de contenu
            seen = set()
            unique_docs = []
            for doc in docs:
                title = doc.metadata.get("titre", "")
                if title not in seen:
                    seen.add(title)
                    unique_docs.append(doc)

        except Exception as e:
            print("Erreur lors de la recherche vectorielle : ", e)
            docs = []
        print(f"Documents trouv√©s : {len(docs)}")
        # Pr√©paration du contexte textuel √† partir des documents trouv√©s
        context_segments = []
        for doc in docs:
            titre_doc = doc.metadata.get("titre", "Formation inconnue")
            extrait = doc.page_content.strip()
            context_segments.append(f"Formation: {titre_doc}\n{extrait}")
        context_text = "\n\n".join(context_segments).strip()
        if not context_text:
            context_text = "(Aucun document pertinent trouv√©)"

        # c. Construction du prompt de g√©n√©ration final avec contexte, historique et entit√©s
        final_prompt_template = PromptTemplate(
            input_variables=["profil", "history", "entities", "context", "question"],
            template=(
                "Tu es un assistant virtuel sp√©cialis√© dans les formations professionnelles. "
                "Tu aides l'utilisateur en r√©pondant de mani√®re claire, utile et engageante √† ses questions.\n\n"
                "{profil}\n"
                "Historique de la conversation :\n{history}\n"
                "Entit√©s mentionn√©es dans le contexte :\n{entities}\n"
                "Contexte documentaire :\n{context}\n"
                "Question de l'utilisateur : {question}\n\n"
                "Consignes de r√©ponse :\n"
                "- Appuie-toi sur le **contexte fourni** (documents et historique) pour formuler ta r√©ponse.\n"
                "- **N'invente pas** d'informations qui ne figurent pas dans le contexte.\n"
                "- Si la question est ambigu√´ ou manque de contexte, propose une reformulation polie pour clarification.\n"
                "- Adopte un ton professionnel et accueillant (style commercial l√©ger) pour mettre en valeur la formation.\n"
                "- R√©ponds en fran√ßais de mani√®re concise et compr√©hensible.\n\n"
                "R√©ponse :"
            )
        )
        # Int√©gration du profil utilisateur s'il est fourni (objectif, niveau, comp√©tences)
        profil_text = ""
        if profile:
            profil_text = (f"Profil de l'utilisateur : "
                           f"Objectif={profile.objective}, Niveau={profile.level}, Comp√©tences={profile.knowledge}")
        # Formatage final du prompt avec toutes les informations
        final_prompt = final_prompt_template.format(
            profil=profil_text,
            history=history_text or "(aucun)",
            entities=entities_text or "Aucune",
            context=context_text or "(vide)",
            question=question
        )
        print("Prompt final envoy√© au LLM:\n", final_prompt)

        # d. G√©n√©ration de la r√©ponse finale √† l'aide du mod√®le LLM (OpenAI GPT-3.5 Turbo)
        try:
            answer = self.llm.predict(final_prompt)
        except Exception as e:
            print("Erreur lors de la g√©n√©ration de la r√©ponse finale: ", e)
            answer = "D√©sol√©, je rencontre des difficult√©s pour r√©pondre √† votre question pour le moment."

        return {"answer": answer}
